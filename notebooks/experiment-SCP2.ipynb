{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0HoaUg3TNasb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchinfo import summary\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c9Dl_rwCPL9E"
      },
      "outputs": [],
      "source": [
        "import pytorch_wavelets as DWT\n",
        "import pywt\n",
        "from pytorch_wavelets import DWT1D, IDWT1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dywpe.ablation.benchmark import PositionalEncodingBenchmark\n",
        "from dywpe.ablation.complete_ablation import run_core_ablation_studies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSdUGRkWb2Nf",
        "outputId": "edd51728-a0fd-4fea-8eb5-2755d0143ac1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fixing labels to be 0-indexed...\n"
          ]
        }
      ],
      "source": [
        "# Fix the labels by subtracting 1 to make them 0-indexed\n",
        "print(\"Fixing labels to be 0-indexed...\")\n",
        "\n",
        "def fix_dataloader_labels(dataloader):\n",
        "    \"\"\"Fix labels in a dataloader by subtracting 1\"\"\"\n",
        "    fixed_data = []\n",
        "    fixed_labels = []\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        fixed_data.append(inputs)\n",
        "        fixed_labels.append(labels - 1)  # Subtract 1 to make 0-indexed\n",
        "\n",
        "    # Concatenate all batches\n",
        "    all_inputs = torch.cat(fixed_data, dim=0)\n",
        "    all_labels = torch.cat(fixed_labels, dim=0)\n",
        "\n",
        "    return all_inputs, all_labels\n",
        "\n",
        "# Extract and fix all data\n",
        "X_train_fixed, y_train_fixed = fix_dataloader_labels(train_loader)\n",
        "X_valid_fixed, y_valid_fixed = fix_dataloader_labels(valid_loader)\n",
        "X_test_fixed, y_test_fixed = fix_dataloader_labels(test_loader)\n",
        "\n",
        "# Create new datasets with fixed labels\n",
        "train_dataset_fixed = TensorDataset(X_train_fixed, y_train_fixed)\n",
        "valid_dataset_fixed = TensorDataset(X_valid_fixed, y_valid_fixed)\n",
        "test_dataset_fixed = TensorDataset(X_test_fixed, y_test_fixed)\n",
        "\n",
        "# Create new data loaders\n",
        "batch_size = 64\n",
        "train_loader_fixed = DataLoader(train_dataset_fixed, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "valid_loader_fixed = DataLoader(valid_dataset_fixed, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "test_loader_fixed = DataLoader(test_dataset_fixed, batch_size=batch_size, shuffle=False, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUARrByrW6HF",
        "outputId": "a3485722-4e78-41f8-ddf1-fd318938b349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading SelfRegulationSCP2 from https://timeseriesclassification.com/aeon-toolkit/SelfRegulationSCP2.zip...\n",
            "Extracting SelfRegulationSCP2...\n",
            "Dataset SelfRegulationSCP2 extracted to datasets/SelfRegulationSCP2.\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension1_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension2_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension3_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension4_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension5_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension6_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension7_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension1_TEST.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension2_TEST.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension3_TEST.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension4_TEST.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension5_TEST.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension6_TEST.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension7_TEST.arff\n",
            "X_train shape: torch.Size([200, 1152, 6]), y_train shape: torch.Size([200])\n",
            "X_valid shape: torch.Size([90, 1152, 6]), y_valid shape: torch.Size([90])\n",
            "X_test shape: torch.Size([90, 1152, 6]), y_test shape: torch.Size([90])\n",
            "Number of classes: 2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.io import arff\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Directory where datasets will be downloaded and extracted\n",
        "DATA_DIR = 'datasets'\n",
        "\n",
        "# Ensure the dataset directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def download_dataset(dataset_name, url):\n",
        "    \"\"\"\n",
        "    Downloads and extracts a zip file containing the dataset.\n",
        "    \"\"\"\n",
        "    zip_path = os.path.join(DATA_DIR, f\"{dataset_name}.zip\")\n",
        "    extract_path = os.path.join(DATA_DIR, dataset_name)\n",
        "\n",
        "    # Download the dataset\n",
        "    print(f\"Downloading {dataset_name} from {url}...\")\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "    # Extract the zip file\n",
        "    print(f\"Extracting {dataset_name}...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    # Remove the zip file after extraction\n",
        "    os.remove(zip_path)\n",
        "    print(f\"Dataset {dataset_name} extracted to {extract_path}.\")\n",
        "    return extract_path\n",
        "\n",
        "def load_arff_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads ARFF file and converts it to a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    print(f\"Loading ARFF file: {file_path}\")\n",
        "    data, meta = arff.loadarff(file_path)\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "def preprocess_data(train_paths, test_paths, batch_size=64):\n",
        "    \"\"\"\n",
        "    Preprocesses the SelfRegulationSCP1 data:\n",
        "    - Loads and combines multiple dimensions from ARFF files.\n",
        "    - Normalizes the features for each dimension.\n",
        "    - Stacks features from different dimensions.\n",
        "    - Converts them into PyTorch tensors.\n",
        "    - Creates DataLoaders for training, validation, and testing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load all training and test dimensions\n",
        "    train_dfs = [load_arff_data(path) for path in train_paths]\n",
        "    test_dfs = [load_arff_data(path) for path in test_paths]\n",
        "\n",
        "    # Separate features and labels for all dimensions\n",
        "    train_features = [df.drop(columns=['cortical']) for df in train_dfs]\n",
        "    test_features = [df.drop(columns=['cortical']) for df in test_dfs]\n",
        "\n",
        "    # Create a label mapping for the two unique class labels\n",
        "    label_mapping = {\n",
        "        b'negativity': 0,\n",
        "        b'positivity': 1\n",
        "    }\n",
        "\n",
        "    # Apply the label mapping to the training and test sets\n",
        "    train_labels = train_dfs[0]['cortical'].apply(lambda x: label_mapping[x]).values\n",
        "    test_labels = test_dfs[0]['cortical'].apply(lambda x: label_mapping[x]).values\n",
        "\n",
        "    # Normalize the features using StandardScaler for each dimension\n",
        "    scalers = [StandardScaler() for _ in range(6)]  # 6 dimensions\n",
        "    train_features_normalized = [scalers[i].fit_transform(train_features[i]) for i in range(6)]\n",
        "    test_features_normalized = [scalers[i].transform(test_features[i]) for i in range(6)]\n",
        "\n",
        "    # Stack all dimensions along a new axis (multivariate time-series)\n",
        "    X_train = np.stack(train_features_normalized, axis=-1)\n",
        "    X_test_full = np.stack(test_features_normalized, axis=-1)\n",
        "\n",
        "    # Split the test data into validation and test sets\n",
        "    X_valid, X_test, y_valid, y_test = train_test_split(X_test_full, test_labels, test_size=0.50, random_state=42)\n",
        "    y_train = train_labels\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.int64)\n",
        "\n",
        "    X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
        "    y_valid = torch.tensor(y_valid, dtype=torch.int64)\n",
        "\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.int64)\n",
        "\n",
        "    # Output dataset shapes\n",
        "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "    print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "    # Return both the DataLoaders and the raw tensors\n",
        "    return train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test\n",
        "\n",
        "# Example usage for downloading, extracting, and preprocessing the SelfRegulationSCP1 dataset\n",
        "if __name__ == \"__main__\":\n",
        "    # URL for the dataset\n",
        "    dataset_name = 'SelfRegulationSCP2'\n",
        "    dataset_url = 'https://timeseriesclassification.com/aeon-toolkit/SelfRegulationSCP2.zip'\n",
        "\n",
        "    # Download and extract the dataset\n",
        "    extract_path = download_dataset(dataset_name, dataset_url)\n",
        "\n",
        "    # Paths for the ARFF files\n",
        "    train_arff_paths = [\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension1_TRAIN.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension2_TRAIN.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension3_TRAIN.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension4_TRAIN.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension5_TRAIN.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension6_TRAIN.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension7_TRAIN.arff')\n",
        "    ]\n",
        "\n",
        "    test_arff_paths = [\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension1_TEST.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension2_TEST.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension3_TEST.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension4_TEST.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension5_TEST.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension6_TEST.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension7_TEST.arff')\n",
        "    ]\n",
        "\n",
        "    # Preprocess the data\n",
        "    train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test = preprocess_data(train_arff_paths, test_arff_paths)\n",
        "\n",
        "    n_classes = len(torch.unique(y_train))\n",
        "\n",
        "    # Output the number of classes\n",
        "    print(f\"Number of classes: {n_classes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoCV3zAQmsCU",
        "outputId": "dc22f5ae-95f1-4b02-ca1b-bc5e42d3f958"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting benchmark on 1 positional encodings...\n",
            "Max epochs per encoding: 100\n",
            "Early stopping patience: 30\n",
            "Test set evaluation: ENABLED\n",
            "\n",
            "==================================================\n",
            "Benchmarking: DYWPE2\n",
            "Early stopping patience: 30\n",
            "==================================================\n",
            "Epoch   1: Train Acc: 0.4740, Val Acc: 0.4219 ⭐ (NEW BEST), Time: 0.94s\n",
            "Epoch   2: Train Acc: 0.5312, Val Acc: 0.6094 ⭐ (NEW BEST), Time: 0.47s\n",
            "Epoch   5: Train Acc: 0.4896, Val Acc: 0.6094, Time: 0.26s\n",
            "Epoch   8: Train Acc: 0.5052, Val Acc: 0.6406 ⭐ (NEW BEST), Time: 0.21s\n",
            "Epoch  10: Train Acc: 0.5260, Val Acc: 0.4219, Time: 0.17s\n",
            "Epoch  15: Train Acc: 0.5677, Val Acc: 0.4688, Time: 0.17s\n",
            "Epoch  20: Train Acc: 0.6354, Val Acc: 0.5156, Time: 0.17s\n",
            "Epoch  25: Train Acc: 0.6510, Val Acc: 0.5312, Time: 0.19s\n",
            "Epoch  30: Train Acc: 0.6354, Val Acc: 0.5469, Time: 0.17s\n",
            "Epoch  35: Train Acc: 0.6406, Val Acc: 0.5156, Time: 0.17s\n",
            "\n",
            "Early stopping triggered after 38 epochs\n",
            "Best validation accuracy: 0.6406 at epoch 8\n",
            "Loaded best model from epoch 8\n",
            "Final Test Acc: 0.6719\n",
            "Training Summary:\n",
            "- Best Val Acc: 0.6406 (epoch 8)\n",
            "- Total Epochs: 38\n",
            "- Early Stopped: Yes\n",
            "- Test Acc: 0.6719\n",
            "\n",
            "====================================================================================================\n",
            "POSITIONAL ENCODING BENCHMARK SUMMARY (with Early Stopping)\n",
            "====================================================================================================\n",
            "Encoding     Best Val Acc Test Acc   Best Epoch Total Epochs Early Stop Avg Time/Epoch \n",
            "-----------------------------------------------------------------------------------------------\n",
            "dywpe2       0.6406       0.6719     8          38           Yes        0.21           \n",
            "\n",
            "Summary Statistics:\n",
            "- Methods that early stopped: 1/1\n",
            "- Average epochs trained: 38.0\n",
            "- Best performing: DYWPE2 (0.6406)\n"
          ]
        }
      ],
      "source": [
        "# Your existing code with enhancements\n",
        "benchmark = PositionalEncodingBenchmark()\n",
        "\n",
        "model_params = {\n",
        "    'input_timesteps': 1152,\n",
        "    'in_channels': 6,\n",
        "    'patch_size': 256,\n",
        "    'embedding_dim': 32,\n",
        "    'num_transformer_layers': 4,\n",
        "    'num_heads': 4,\n",
        "    'dim_feedforward': 256,\n",
        "    'dropout': 0.2,\n",
        "    'num_classes': 2\n",
        "}\n",
        "\n",
        "models = benchmark.run_full_benchmark(\n",
        "    model_params,\n",
        "    train_loader,\n",
        "    valid_loader,\n",
        "    test_loader,\n",
        "    encodings=['dywpe'],\n",
        "    n_epochs=100,  # Maximum epochs\n",
        "    early_stopping_patience=30,  # Stop if no improvement for 15 epochs\n",
        "    min_delta=0.001,  # Minimum improvement threshold\n",
        "    save_models=True,  # Save best models\n",
        "    models_dir=\"./saved_models\"  # Where to save\n",
        ")\n",
        "\n",
        "# The summary will now show early stopping information\n",
        "benchmark.print_summary()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
