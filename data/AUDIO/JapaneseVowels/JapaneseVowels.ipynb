{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb30b345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading JapaneseVowels from https://timeseriesclassification.com/aeon-toolkit/JapaneseVowels.zip...\n",
      "Extracting JapaneseVowels...\n",
      "Dataset JapaneseVowels extracted to datasets/JapaneseVowels.\n",
      "Available files: ['JapaneseVowels_eq_TEST.ts', 'JapaneseVowels_eq_TRAIN.ts', 'JapaneseVowels_TRAIN.ts', 'JapaneseVowels_TEST.ts']\n",
      "Using training file: JapaneseVowels_TRAIN.ts\n",
      "Using test file: JapaneseVowels_TEST.ts\n",
      "Found 270 samples\n",
      "Maximum time length: 26\n",
      "Number of dimensions: 12\n",
      "Final data shape: (270, 26, 12)\n",
      "Final labels shape: (270,)\n",
      "Found 370 samples\n",
      "Maximum time length: 29\n",
      "Number of dimensions: 12\n",
      "Final data shape: (370, 29, 12)\n",
      "Final labels shape: (370,)\n",
      "Train time points: 26, Test time points: 29\n",
      "Different time lengths detected. Padding to match the maximum length.\n",
      "Padded to 29 time points\n",
      "Final shapes - Train: (270, 29, 12), Test: (370, 29, 12)\n",
      "\n",
      "Final Results:\n",
      "Number of classes: 9\n",
      "X_train shape: torch.Size([270, 29, 12]), y_train shape: torch.Size([270])\n",
      "X_valid shape: torch.Size([185, 29, 12]), y_valid shape: torch.Size([185])\n",
      "X_test shape: torch.Size([185, 29, 12]), y_test shape: torch.Size([185])\n",
      "Number of dimensions: 12\n",
      "Time points: 29\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Directory where datasets will be downloaded and extracted\n",
    "DATA_DIR = 'datasets'\n",
    "\n",
    "# Ensure the dataset directory exists\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "def download_dataset(dataset_name, url):\n",
    "    \"\"\"\n",
    "    Downloads and extracts a zip file containing the dataset.\n",
    "    \"\"\"\n",
    "    zip_path = os.path.join(DATA_DIR, f\"{dataset_name}.zip\")\n",
    "    extract_path = os.path.join(DATA_DIR, dataset_name)\n",
    "\n",
    "    # Download the dataset\n",
    "    print(f\"Downloading {dataset_name} from {url}...\")\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "\n",
    "    # Extract the zip file\n",
    "    print(f\"Extracting {dataset_name}...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "\n",
    "    # Remove the zip file after extraction\n",
    "    os.remove(zip_path)\n",
    "    print(f\"Dataset {dataset_name} extracted to {extract_path}.\")\n",
    "    return extract_path\n",
    "\n",
    "def load_japanese_vowels_ts(file_path):\n",
    "    \"\"\"\n",
    "    Load JapaneseVowels .ts file which has a specific multivariate format.\n",
    "    Each line contains multiple time series separated by colons, followed by the label.\n",
    "    Format: series1:series2:...:seriesN:label\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        is_metadata = True\n",
    "        for line_num, line in enumerate(file):\n",
    "            line = line.strip()\n",
    "\n",
    "            # Skip metadata until @data\n",
    "            if is_metadata:\n",
    "                if line.lower() == \"@data\":\n",
    "                    is_metadata = False\n",
    "                continue\n",
    "\n",
    "            # Skip empty lines\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Split by colon to separate different time series and label\n",
    "                parts = line.split(':')\n",
    "\n",
    "                if len(parts) < 2:\n",
    "                    print(f\"Skipping line {line_num}: insufficient parts\")\n",
    "                    continue\n",
    "\n",
    "                # Last part is the label\n",
    "                label = int(parts[-1])\n",
    "\n",
    "                # All other parts are time series (one per dimension)\n",
    "                time_series_parts = parts[:-1]\n",
    "\n",
    "                # Convert each time series part to arrays\n",
    "                time_series_arrays = []\n",
    "                for ts_part in time_series_parts:\n",
    "                    if ts_part.strip():  # Skip empty parts\n",
    "                        ts_values = [float(x) for x in ts_part.split(',')]\n",
    "                        time_series_arrays.append(ts_values)\n",
    "\n",
    "                if len(time_series_arrays) == 0:\n",
    "                    print(f\"Skipping line {line_num}: no valid time series data\")\n",
    "                    continue\n",
    "\n",
    "                # Stack the time series arrays to create multivariate data\n",
    "                # Shape: (time_points, n_dimensions)\n",
    "                multivariate_series = np.array(time_series_arrays).T\n",
    "\n",
    "                data.append(multivariate_series)\n",
    "                labels.append(label)\n",
    "\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"Skipping line {line_num}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    if len(data) == 0:\n",
    "        raise ValueError(\"No valid data found in .ts file\")\n",
    "\n",
    "    # Find the maximum time length for padding\n",
    "    max_time_length = max(series.shape[0] for series in data)\n",
    "    n_dimensions = data[0].shape[1]\n",
    "\n",
    "    print(f\"Found {len(data)} samples\")\n",
    "    print(f\"Maximum time length: {max_time_length}\")\n",
    "    print(f\"Number of dimensions: {n_dimensions}\")\n",
    "\n",
    "    # Pad all series to the same length\n",
    "    padded_data = []\n",
    "    for series in data:\n",
    "        if series.shape[0] < max_time_length:\n",
    "            # Pad with the last value\n",
    "            padding_needed = max_time_length - series.shape[0]\n",
    "            last_values = series[-1:].repeat(padding_needed, axis=0)\n",
    "            padded_series = np.vstack([series, last_values])\n",
    "        else:\n",
    "            padded_series = series\n",
    "        padded_data.append(padded_series)\n",
    "\n",
    "    # Convert to numpy array: (n_samples, time_points, n_dimensions)\n",
    "    data_array = np.array(padded_data)\n",
    "    labels_array = np.array(labels)\n",
    "\n",
    "    print(f\"Final data shape: {data_array.shape}\")\n",
    "    print(f\"Final labels shape: {labels_array.shape}\")\n",
    "\n",
    "    return data_array, labels_array\n",
    "\n",
    "# Dataset information\n",
    "dataset_name = 'JapaneseVowels'\n",
    "dataset_url = 'https://timeseriesclassification.com/aeon-toolkit/JapaneseVowels.zip'\n",
    "\n",
    "# Download and extract the dataset\n",
    "extract_path = download_dataset(dataset_name, dataset_url)\n",
    "\n",
    "# Check available files\n",
    "files = os.listdir(extract_path)\n",
    "print(f\"Available files: {files}\")\n",
    "\n",
    "# Try different file variations\n",
    "possible_train_files = [\n",
    "    'JapaneseVowels_TRAIN.ts',\n",
    "    'JapaneseVowels_eq_TRAIN.ts'\n",
    "]\n",
    "\n",
    "possible_test_files = [\n",
    "    'JapaneseVowels_TEST.ts',\n",
    "    'JapaneseVowels_eq_TEST.ts'\n",
    "]\n",
    "\n",
    "# Find the correct files\n",
    "train_file = None\n",
    "test_file = None\n",
    "\n",
    "for fname in possible_train_files:\n",
    "    if fname in files:\n",
    "        train_file = os.path.join(extract_path, fname)\n",
    "        print(f\"Using training file: {fname}\")\n",
    "        break\n",
    "\n",
    "for fname in possible_test_files:\n",
    "    if fname in files:\n",
    "        test_file = os.path.join(extract_path, fname)\n",
    "        print(f\"Using test file: {fname}\")\n",
    "        break\n",
    "\n",
    "if train_file is None or test_file is None:\n",
    "    print(\"Could not find appropriate .ts files\")\n",
    "    print(\"Available files:\", files)\n",
    "    exit()\n",
    "\n",
    "# Load the train and test datasets\n",
    "X_train, y_train = load_japanese_vowels_ts(train_file)\n",
    "X_test, y_test = load_japanese_vowels_ts(test_file)\n",
    "\n",
    "# Check if train and test have different time lengths\n",
    "n_train_samples, train_time_points, n_dimensions = X_train.shape\n",
    "n_test_samples, test_time_points, _ = X_test.shape\n",
    "\n",
    "print(f\"Train time points: {train_time_points}, Test time points: {test_time_points}\")\n",
    "\n",
    "if train_time_points != test_time_points:\n",
    "    print(\"Different time lengths detected. Padding to match the maximum length.\")\n",
    "    max_time_points = max(train_time_points, test_time_points)\n",
    "\n",
    "    # Pad training data if needed\n",
    "    if train_time_points < max_time_points:\n",
    "        padding_needed = max_time_points - train_time_points\n",
    "        # Pad with last values\n",
    "        last_values = X_train[:, -1:, :].repeat(padding_needed, axis=1)\n",
    "        X_train = np.concatenate([X_train, last_values], axis=1)\n",
    "\n",
    "    # Pad test data if needed\n",
    "    if test_time_points < max_time_points:\n",
    "        padding_needed = max_time_points - test_time_points\n",
    "        # Pad with last values\n",
    "        last_values = X_test[:, -1:, :].repeat(padding_needed, axis=1)\n",
    "        X_test = np.concatenate([X_test, last_values], axis=1)\n",
    "\n",
    "    time_points = max_time_points\n",
    "    print(f\"Padded to {time_points} time points\")\n",
    "else:\n",
    "    time_points = train_time_points\n",
    "\n",
    "# Update shapes after potential padding\n",
    "n_train_samples, time_points, n_dimensions = X_train.shape\n",
    "n_test_samples = X_test.shape[0]\n",
    "\n",
    "print(f\"Final shapes - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Normalize the features\n",
    "# Reshape for normalization: (n_samples * time_points, n_dimensions)\n",
    "X_train_reshaped = X_train.reshape(-1, n_dimensions)\n",
    "X_test_reshaped = X_test.reshape(-1, n_dimensions)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_flat = scaler.fit_transform(X_train_reshaped)\n",
    "X_test_scaled_flat = scaler.transform(X_test_reshaped)\n",
    "\n",
    "# Reshape back to original shape\n",
    "X_train_scaled = X_train_scaled_flat.reshape(n_train_samples, time_points, n_dimensions)\n",
    "X_test_scaled = X_test_scaled_flat.reshape(n_test_samples, time_points, n_dimensions)\n",
    "\n",
    "# Split the test data into validation and test sets\n",
    "X_valid_scaled, X_test_scaled, y_valid, y_test = train_test_split(\n",
    "    X_test_scaled, y_test, test_size=0.50, random_state=42\n",
    ")\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
    "\n",
    "X_valid_tensor = torch.tensor(X_valid_scaled, dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(y_valid, dtype=torch.int64)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Calculate the number of unique classes\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "# Print the shapes and number of classes\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "print(f\"X_train shape: {X_train_tensor.shape}, y_train shape: {y_train_tensor.shape}\")\n",
    "print(f\"X_valid shape: {X_valid_tensor.shape}, y_valid shape: {y_valid_tensor.shape}\")\n",
    "print(f\"X_test shape: {X_test_tensor.shape}, y_test shape: {y_test_tensor.shape}\")\n",
    "print(f\"Number of dimensions: {n_dimensions}\")\n",
    "print(f\"Time points: {time_points}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
